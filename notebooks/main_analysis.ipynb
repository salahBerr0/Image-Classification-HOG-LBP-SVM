{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190319c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Image Classification: HOG vs LBP with SVM\n",
    "# \n",
    "# **Complete Analysis Notebook**\n",
    "# \n",
    "# This notebook performs a comparative analysis of HOG (Histogram of Oriented Gradients) and LBP (Local Binary Patterns) feature extractors for image classification using Support Vector Machines (SVM).\n",
    "# \n",
    "# ## Dataset\n",
    "# - 5 classes: City, Face, Green, Office, Sea\n",
    "# - Training: 695 images (~140 per class)\n",
    "# - Testing: 150 images (30 per class)\n",
    "# \n",
    "# ## Methodology\n",
    "# 1. Load and preprocess images\n",
    "# 2. Extract HOG and LBP features\n",
    "# 3. Train SVM classifiers\n",
    "# 4. Evaluate and compare performance\n",
    "# \n",
    "# *Note: This notebook imports the modular code from the `src/` directory.*\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup and Imports\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to Python path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import the modular components\n",
    "from config import Config\n",
    "from data_loader import DataLoader\n",
    "from feature_extractors import FeatureExtractor\n",
    "from classifiers import ModelTrainer\n",
    "from visualizer import Visualizer\n",
    "\n",
    "# Standard imports for data manipulation and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All imports completed successfully!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Initialize Configuration and Components\n",
    "\n",
    "# %%\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Update dataset path for your local environment\n",
    "config.DATASET_PATH = 'data/'  # Change this to your actual dataset path\n",
    "\n",
    "# Initialize the components\n",
    "data_loader = DataLoader(config)\n",
    "feature_extractor = FeatureExtractor(config)\n",
    "\n",
    "print(\"ðŸ”„ Components initialized:\")\n",
    "print(f\"   - Dataset path: {config.DATASET_PATH}\")\n",
    "print(f\"   - Image size: {config.IMG_SIZE}\")\n",
    "print(f\"   - Classes: {data_loader.classes}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Load and Explore Dataset\n",
    "\n",
    "# %%\n",
    "print(\"ðŸ“‚ Loading dataset...\")\n",
    "X_train, X_test, y_train, y_test = data_loader.load_dataset()\n",
    "\n",
    "# Display dataset information\n",
    "print(\"\\nðŸ“Š Dataset Summary:\")\n",
    "print(f\"   Training images: {X_train.shape[0]}\")\n",
    "print(f\"   Test images: {X_test.shape[0]}\")\n",
    "print(f\"   Image shape: {X_train[0].shape}\")\n",
    "print(f\"   Number of classes: {len(data_loader.classes)}\")\n",
    "\n",
    "# Check class distribution\n",
    "train_counts = np.bincount(y_train)\n",
    "test_counts = np.bincount(y_test)\n",
    "\n",
    "print(\"\\nðŸŽ¯ Class Distribution:\")\n",
    "for i, class_name in enumerate(data_loader.classes):\n",
    "    print(f\"   {class_name}: Train={train_counts[i]}, Test={test_counts[i]}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Visualize Sample Images\n",
    "\n",
    "# %%\n",
    "# Initialize visualizer\n",
    "visualizer = Visualizer(data_loader)\n",
    "\n",
    "# Plot sample images from each class\n",
    "print(\"ðŸ–¼ï¸ Displaying sample images from each class...\")\n",
    "visualizer.plot_sample_images(X_train, y_train, num_samples=3)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Feature Extraction\n",
    "\n",
    "# %%\n",
    "print(\"ðŸ” Extracting HOG features...\")\n",
    "X_train_hog, hog_train_vis = feature_extractor.extract_hog_features(X_train)\n",
    "X_test_hog, hog_test_vis = feature_extractor.extract_hog_features(X_test)\n",
    "\n",
    "print(\"\\nðŸ” Extracting LBP features...\")\n",
    "X_train_lbp, lbp_train_vis = feature_extractor.extract_lbp_features(X_train)\n",
    "X_test_lbp, lbp_test_vis = feature_extractor.extract_lbp_features(X_test)\n",
    "\n",
    "print(\"\\nðŸ“Š Feature Dimensions:\")\n",
    "print(f\"   HOG features - Train: {X_train_hog.shape}, Test: {X_test_hog.shape}\")\n",
    "print(f\"   LBP features - Train: {X_train_lbp.shape}, Test: {X_test_lbp.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Visualize Extracted Features\n",
    "\n",
    "# %%\n",
    "print(\"ðŸŽ¨ Visualizing HOG and LBP features...\")\n",
    "visualizer.visualize_features(X_train, hog_train_vis, lbp_train_vis, num_examples=3)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Model Training - HOG + SVM\n",
    "\n",
    "# %%\n",
    "# Initialize model trainer\n",
    "trainer = ModelTrainer(config, data_loader)\n",
    "\n",
    "print(\"ðŸ¤– Training HOG + SVM model...\")\n",
    "hog_results = trainer.train_svm_model(X_train_hog, X_test_hog, y_train, y_test, \"HOG\")\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"\\nðŸ“ˆ HOG + SVM Confusion Matrix:\")\n",
    "visualizer.plot_confusion_matrix(hog_results['confusion_matrix'], \"HOG + SVM\")\n",
    "\n",
    "# Save the model\n",
    "os.makedirs('../results/models', exist_ok=True)\n",
    "joblib.dump(hog_results['model'], '../results/models/hog_svm_model.pkl')\n",
    "print(\"ðŸ’¾ HOG model saved to: ../results/models/hog_svm_model.pkl\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Model Training - LBP + SVM\n",
    "\n",
    "# %%\n",
    "print(\"ðŸ¤– Training LBP + SVM model...\")\n",
    "lbp_results = trainer.train_svm_model(X_train_lbp, X_test_lbp, y_train, y_test, \"LBP\")\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"\\nðŸ“ˆ LBP + SVM Confusion Matrix:\")\n",
    "visualizer.plot_confusion_matrix(lbp_results['confusion_matrix'], \"LBP + SVM\")\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(lbp_results['model'], '../results/models/lbp_svm_model.pkl')\n",
    "print(\"ðŸ’¾ LBP model saved to: ../results/models/lbp_svm_model.pkl\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Performance Comparison\n",
    "\n",
    "# %%\n",
    "print(\"ðŸ“Š Comparing HOG vs LBP performance...\")\n",
    "df_comparison = visualizer.compare_results(hog_results, lbp_results)\n",
    "\n",
    "# Save comparison results\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "results = {\n",
    "    'hog_results': hog_results,\n",
    "    'lbp_results': lbp_results,\n",
    "    'comparison': df_comparison.to_dict(),\n",
    "    'config': {\n",
    "        'hog_orientations': config.HOG_ORIENTATIONS,\n",
    "        'hog_pixels_per_cell': config.HOG_PIXELS_PER_CELL,\n",
    "        'hog_cells_per_block': config.HOG_CELLS_PER_BLOCK,\n",
    "        'lbp_radius': config.LBP_RADIUS,\n",
    "        'lbp_n_points': config.LBP_N_POINTS,\n",
    "        'svm_kernel': config.SVM_KERNEL,\n",
    "        'svm_c': config.SVM_C\n",
    "    }\n",
    "}\n",
    "\n",
    "joblib.dump(results, '../results/classification_results.pkl')\n",
    "print(\"ðŸ’¾ Complete results saved to: ../results/classification_results.pkl\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Additional Analysis and Visualizations\n",
    "\n",
    "# %%\n",
    "# Create a detailed comparison table\n",
    "print(\"ðŸ“‹ Detailed Performance Metrics:\")\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'HOG+SVM': [\n",
    "        hog_results['accuracy'],\n",
    "        hog_results['precision'],\n",
    "        hog_results['recall'],\n",
    "        hog_results['f1']\n",
    "    ],\n",
    "    'LBP+SVM': [\n",
    "        lbp_results['accuracy'],\n",
    "        lbp_results['precision'],\n",
    "        lbp_results['recall'],\n",
    "        lbp_results['f1']\n",
    "    ],\n",
    "    'Difference': [\n",
    "        hog_results['accuracy'] - lbp_results['accuracy'],\n",
    "        hog_results['precision'] - lbp_results['precision'],\n",
    "        hog_results['recall'] - lbp_results['recall'],\n",
    "        hog_results['f1'] - lbp_results['f1']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize feature distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# HOG feature distribution\n",
    "axes[0].hist(X_train_hog.flatten(), bins=50, alpha=0.7, color='skyblue')\n",
    "axes[0].set_title('HOG Feature Distribution')\n",
    "axes[0].set_xlabel('Feature Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# LBP feature distribution\n",
    "axes[1].hist(X_train_lbp.flatten(), bins=50, alpha=0.7, color='lightcoral')\n",
    "axes[1].set_title('LBP Feature Distribution')\n",
    "axes[1].set_xlabel('Feature Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"ðŸ’¾ Feature distributions saved to: ../results/figures/feature_distributions.png\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 11. Test on Individual Samples\n",
    "\n",
    "# %%\n",
    "def test_single_image(image_path, model_type='hog'):\n",
    "    \"\"\"Test a single image with the trained model\"\"\"\n",
    "    from PIL import Image\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    img = Image.open(image_path)\n",
    "    img = np.array(img)\n",
    "    \n",
    "    if len(img.shape) == 2:  # Grayscale\n",
    "        img = np.stack([img, img, img], axis=-1)\n",
    "    \n",
    "    # Resize to match training size\n",
    "    img = cv2.resize(img, config.IMG_SIZE)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Extract features\n",
    "    if model_type == 'hog':\n",
    "        features, _ = feature_extractor.extract_hog_features([img])\n",
    "        model = hog_results['model']\n",
    "    else:\n",
    "        features, _ = feature_extractor.extract_lbp_features([img])\n",
    "        model = lbp_results['model']\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(features)[0]\n",
    "    probability = model.predict_proba(features)[0]\n",
    "    \n",
    "    # Display results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('Input Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Prediction probabilities\n",
    "    classes = data_loader.classes\n",
    "    bars = axes[1].barh(classes, probability)\n",
    "    axes[1].set_xlabel('Probability')\n",
    "    axes[1].set_title(f'Prediction: {classes[prediction]} ({probability[prediction]:.2%})')\n",
    "    axes[1].axvline(x=probability[prediction], color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Color the predicted class bar\n",
    "    bars[prediction].set_color('red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return classes[prediction], probability\n",
    "\n",
    "print(\"ðŸ” Single image test function ready!\")\n",
    "print(\"To test an image, run: test_single_image('path/to/image.jpg', model_type='hog')\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 12. Summary and Conclusions\n",
    "\n",
    "# %%\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ“‹ PROJECT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ† Best Performing Method: {'HOG+SVM' if hog_results['accuracy'] > lbp_results['accuracy'] else 'LBP+SVM'}\")\n",
    "print(f\"   Accuracy: {max(hog_results['accuracy'], lbp_results['accuracy']):.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Key Findings:\")\n",
    "print(f\"   1. HOG accuracy: {hog_results['accuracy']:.4f}\")\n",
    "print(f\"   2. LBP accuracy: {lbp_results['accuracy']:.4f}\")\n",
    "print(f\"   3. Performance difference: {abs(hog_results['accuracy'] - lbp_results['accuracy']):.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ”§ Parameters Used:\")\n",
    "print(f\"   - HOG: {config.HOG_ORIENTATIONS} orientations, {config.HOG_PIXELS_PER_CELL} pixels/cell\")\n",
    "print(f\"   - LBP: Radius={config.LBP_RADIUS}, Points={config.LBP_N_POINTS}\")\n",
    "print(f\"   - SVM: Kernel={config.SVM_KERNEL}, C={config.SVM_C}\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Generated Files:\")\n",
    "print(\"   - models/hog_svm_model.pkl\")\n",
    "print(\"   - models/lbp_svm_model.pkl\")\n",
    "print(\"   - classification_results.pkl\")\n",
    "print(\"   - figures/feature_visualization.png\")\n",
    "print(\"   - figures/feature_distributions.png\")\n",
    "\n",
    "print(f\"\\nðŸš€ Next Steps:\")\n",
    "print(\"   1. Experiment with different parameters\")\n",
    "print(\"   2. Try combining HOG and LBP features\")\n",
    "print(\"   3. Test on new image categories\")\n",
    "print(\"   4. Create a web interface for real-time classification\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 13. Save Notebook Results (Optional)\n",
    "\n",
    "# %%\n",
    "# Save the complete notebook state for reproducibility\n",
    "notebook_results = {\n",
    "    'X_train_shape': X_train.shape,\n",
    "    'X_test_shape': X_test.shape,\n",
    "    'hog_train_shape': X_train_hog.shape,\n",
    "    'lbp_train_shape': X_train_lbp.shape,\n",
    "    'hog_accuracy': hog_results['accuracy'],\n",
    "    'lbp_accuracy': lbp_results['accuracy'],\n",
    "    'config': config.__dict__\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for nice display\n",
    "results_df = pd.DataFrame([notebook_results])\n",
    "print(\"ðŸ“Š Final Results Summary:\")\n",
    "print(results_df.T)  # Transpose for better readability\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('../results/notebook_summary.csv', index=False)\n",
    "print(\"ðŸ’¾ Notebook summary saved to: ../results/notebook_summary.csv\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "# \n",
    "# ## How to Use This Notebook\n",
    "# \n",
    "# 1. **Setup**: Update `config.DATASET_PATH` in Section 2 to your dataset location\n",
    "# 2. **Run All Cells**: Use \"Runtime\" â†’ \"Run all\" (in Colab) or \"Kernel\" â†’ \"Restart & Run All\" (in Jupyter)\n",
    "# 3. **View Results**: All outputs will be displayed inline\n",
    "# 4. **Generated Files**: Check the `results/` directory for saved models and visualizations\n",
    "# \n",
    "# ## Files Structure After Running:\n",
    "# ```\n",
    "# project/\n",
    "# â”œâ”€â”€ notebooks/main_analysis.ipynb    # This notebook\n",
    "# â”œâ”€â”€ src/                            # Modular code\n",
    "# â”œâ”€â”€ data/                           # Dataset\n",
    "# â””â”€â”€ results/                        # Generated files\n",
    "#     â”œâ”€â”€ models/                     # Trained models\n",
    "#     â”œâ”€â”€ figures/                    # Visualizations\n",
    "#     â””â”€â”€ classification_results.pkl  # All results\n",
    "# ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
