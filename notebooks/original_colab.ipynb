{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d487824",
   "metadata": {},
   "source": [
    "##### Image Classification: HOG vs LBP with SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5507e2",
   "metadata": {},
   "source": [
    "# âš ï¸This notebook was developed in Google Colab\"\n",
    "##### This is for **reference/history only** - shows your experimentation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d2540",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Install required packages\n",
    "!pip install opendatasets scikit-image seaborn joblib > /dev/null\n",
    "\n",
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import feature, exposure\n",
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import seaborn as sns\n",
    "import os\n",
    "from google.colab import drive\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import opendatasets as od\n",
    "\n",
    "print(\"âœ… All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c0922",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# MOUNT GOOGLE DRIVE AND SETUP DATASET\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Configuration class\n",
    "class Config:\n",
    "    DATASET_PATH = '/content/drive/MyDrive/ML/hog_lbp_Dataset'\n",
    "\n",
    "    # HOG parameters\n",
    "    HOG_ORIENTATIONS = 9\n",
    "    HOG_PIXELS_PER_CELL = (8, 8)\n",
    "    HOG_CELLS_PER_BLOCK = (2, 2)\n",
    "\n",
    "    # LBP parameters\n",
    "    LBP_RADIUS = 3\n",
    "    LBP_N_POINTS = 24\n",
    "    LBP_METHOD = 'uniform'\n",
    "\n",
    "    # SVM parameters\n",
    "    SVM_C = 1.0\n",
    "    SVM_KERNEL = 'rbf'  # Options: 'rbf', 'sigmoid'\n",
    "    SVM_GAMMA = 'scale'\n",
    "\n",
    "    # Cross-validation\n",
    "    K_FOLDS = 5\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "    # Image size for resizing\n",
    "    IMG_SIZE = (128, 128)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Check if dataset path exists\n",
    "if os.path.exists(config.DATASET_PATH):\n",
    "    print(f\"âœ… Dataset found at: {config.DATASET_PATH}\")\n",
    "else:\n",
    "    print(f\"âŒ Dataset not found at: {config.DATASET_PATH}\")\n",
    "    print(\"Please update the DATASET_PATH in Config to match your Google Drive folder structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1afb29d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# DATA LOADER CLASS\n",
    "class DataLoader:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.classes = ['city', 'face', 'green', 'office', 'sea']\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.idx_to_class = {idx: cls for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "    def load_images_from_folder(self, folder_path):\n",
    "        \"\"\"Load images from a specific folder\"\"\"\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(folder_path, class_name)\n",
    "            if not os.path.exists(class_path):\n",
    "                print(f\"âš ï¸ Warning: {class_path} does not exist\")\n",
    "                continue\n",
    "\n",
    "            print(f\"ğŸ“ Loading {class_name} images...\")\n",
    "            class_images = []\n",
    "            for img_file in tqdm(os.listdir(class_path)):\n",
    "                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(class_path, img_file)\n",
    "                    try:\n",
    "                        # Read image\n",
    "                        img = cv2.imread(img_path)\n",
    "                        if img is not None:\n",
    "                            # Resize and convert color format\n",
    "                            img = cv2.resize(img, self.config.IMG_SIZE)\n",
    "                            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                            class_images.append(img)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {img_path}: {e}\")\n",
    "\n",
    "            images.extend(class_images)\n",
    "            labels.extend([self.class_to_idx[class_name]] * len(class_images))\n",
    "            print(f\"âœ… Loaded {len(class_images)} {class_name} images\")\n",
    "\n",
    "        return np.array(images), np.array(labels)\n",
    "\n",
    "    def load_dataset(self):\n",
    "        \"\"\"Load both train and test datasets\"\"\"\n",
    "        train_path = os.path.join(self.config.DATASET_PATH, 'train')\n",
    "        test_path = os.path.join(self.config.DATASET_PATH, 'test')\n",
    "\n",
    "        print(\"ğŸš€ Loading training dataset...\")\n",
    "        X_train, y_train = self.load_images_from_folder(train_path)\n",
    "\n",
    "        print(\"ğŸš€ Loading test dataset...\")\n",
    "        X_test, y_test = self.load_images_from_folder(test_path)\n",
    "\n",
    "        print(f\"\\nğŸ“Š Dataset Summary:\")\n",
    "        print(f\"Training set: {X_train.shape[0]} images\")\n",
    "        print(f\"Test set: {X_test.shape[0]} images\")\n",
    "        print(f\"Image shape: {X_train[0].shape}\")\n",
    "        print(f\"Classes: {self.classes}\")\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = DataLoader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c7dfc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "X_train, X_test, y_train, y_test = data_loader.load_dataset()\n",
    "# Display dataset information\n",
    "print(f\"ğŸ“ˆ Training data shape: {X_train.shape}\")\n",
    "print(f\"ğŸ“ˆ Test data shape: {X_test.shape}\")\n",
    "print(f\"ğŸ¯ Training labels distribution: {np.unique(y_train, return_counts=True)}\")\n",
    "print(f\"ğŸ¯ Test labels distribution: {np.unique(y_test, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e383e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize some sample images from each class\n",
    "def plot_sample_images(images, labels, num_samples=3):\n",
    "    fig, axes = plt.subplots(len(data_loader.classes), num_samples, figsize=(15, 12))\n",
    "\n",
    "    for class_idx, class_name in enumerate(data_loader.classes):\n",
    "        # Get indices of current class\n",
    "        class_indices = np.where(labels == class_idx)[0]\n",
    "\n",
    "        # Select random samples\n",
    "        if len(class_indices) > 0:\n",
    "            sample_indices = np.random.choice(class_indices, num_samples, replace=False)\n",
    "\n",
    "            for i, idx in enumerate(sample_indices):\n",
    "                axes[class_idx, i].imshow(images[idx])\n",
    "                axes[class_idx, i].set_title(f'{class_name}\\nIdx: {idx}')\n",
    "                axes[class_idx, i].axis('off')\n",
    "        else:\n",
    "            for i in range(num_samples):\n",
    "                axes[class_idx, i].axis('off')\n",
    "                axes[class_idx, i].set_title(f'{class_name}\\nNo images')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Sample Images from Each Class', fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "plot_sample_images(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff62604d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# FEATURE EXTRACTION CLASSES\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def extract_hog_features(self, images):\n",
    "        \"\"\"Extract HOG features from images\"\"\"\n",
    "        hog_features = []\n",
    "        hog_images = []\n",
    "\n",
    "        print(\"ğŸ” Extracting HOG features...\")\n",
    "        for img in tqdm(images):\n",
    "            # Convert to grayscale for HOG\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "            # Extract HOG features and visualization\n",
    "            features, hog_image = feature.hog(\n",
    "                gray,\n",
    "                orientations=self.config.HOG_ORIENTATIONS,\n",
    "                pixels_per_cell=self.config.HOG_PIXELS_PER_CELL,\n",
    "                cells_per_block=self.config.HOG_CELLS_PER_BLOCK,\n",
    "                visualize=True,\n",
    "                block_norm='L2-Hys'\n",
    "            )\n",
    "\n",
    "            hog_features.append(features)\n",
    "            hog_images.append(hog_image)\n",
    "\n",
    "        return np.array(hog_features), hog_images\n",
    "\n",
    "    def extract_lbp_features(self, images):\n",
    "        \"\"\"Extract LBP features from RGB images\"\"\"\n",
    "        lbp_features = []\n",
    "        lbp_images = []\n",
    "\n",
    "        print(\"ğŸ” Extracting LBP features...\")\n",
    "        for img in tqdm(images):\n",
    "            # Extract LBP for each channel and concatenate\n",
    "            channel_features = []\n",
    "            channel_lbp_images = []\n",
    "\n",
    "            for channel in range(3):\n",
    "                channel_img = img[:, :, channel]\n",
    "\n",
    "                # Compute LBP\n",
    "                lbp = local_binary_pattern(\n",
    "                    channel_img,\n",
    "                    self.config.LBP_N_POINTS,\n",
    "                    self.config.LBP_RADIUS,\n",
    "                    method=self.config.LBP_METHOD\n",
    "                )\n",
    "\n",
    "                # Compute histogram\n",
    "                n_bins = self.config.LBP_N_POINTS + 2\n",
    "                hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins))\n",
    "\n",
    "                # Normalize histogram\n",
    "                hist = hist.astype(\"float\")\n",
    "                hist /= (hist.sum() + 1e-6)  # Avoid division by zero\n",
    "\n",
    "                channel_features.extend(hist)\n",
    "                channel_lbp_images.append(lbp)\n",
    "\n",
    "            lbp_features.append(channel_features)\n",
    "            lbp_images.append(channel_lbp_images)\n",
    "\n",
    "        return np.array(lbp_features), lbp_images\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = FeatureExtractor(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99b04b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# EXTRACT FEATURES\n",
    "\n",
    "# Extract HOG features\n",
    "X_train_hog, hog_train_vis = feature_extractor.extract_hog_features(X_train)\n",
    "X_test_hog, hog_test_vis = feature_extractor.extract_hog_features(X_test)\n",
    "\n",
    "# Extract LBP features\n",
    "X_train_lbp, lbp_train_vis = feature_extractor.extract_lbp_features(X_train)\n",
    "X_test_lbp, lbp_test_vis = feature_extractor.extract_lbp_features(X_test)\n",
    "\n",
    "print(f\"\\nğŸ“Š Feature Extraction Complete:\")\n",
    "print(f\"HOG features - Train: {X_train_hog.shape}, Test: {X_test_hog.shape}\")\n",
    "print(f\"LBP features - Train: {X_train_lbp.shape}, Test: {X_test_lbp.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd13538",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZE FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_features(original_images, hog_images, lbp_images, num_examples=3):\n",
    "    \"\"\"Visualize original images with HOG and LBP features\"\"\"\n",
    "\n",
    "    # HOG Visualization\n",
    "    print(\"ğŸ–¼ï¸ Visualizing HOG Features...\")\n",
    "    fig, axes = plt.subplots(num_examples, 3, figsize=(15, 5*num_examples))\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        # Original image\n",
    "        axes[i, 0].imshow(original_images[i])\n",
    "        axes[i, 0].set_title(f'Original Image\\nClass: {data_loader.idx_to_class[y_train[i]]}')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # HOG image\n",
    "        axes[i, 1].imshow(hog_images[i], cmap='gray')\n",
    "        axes[i, 1].set_title('HOG Features')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        # LBP visualization (first channel)\n",
    "        axes[i, 2].imshow(lbp_images[i][0], cmap='gray')\n",
    "        axes[i, 2].set_title('LBP Features (Channel 0)')\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize features\n",
    "visualize_features(X_train, hog_train_vis, lbp_train_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce4edb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# MODEL TRAINING AND EVALUATION CLASS\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config, data_loader):\n",
    "        self.config = config\n",
    "        self.data_loader = data_loader\n",
    "\n",
    "    def train_svm_model(self, X_train, X_test, y_train, y_test, feature_name):\n",
    "        \"\"\"Train and evaluate SVM model\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸš€ Training SVM with {feature_name} features\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Create SVM pipeline with standardization\n",
    "        svm_pipeline = make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SVC(\n",
    "                C=self.config.SVM_C,\n",
    "                kernel=self.config.SVM_KERNEL,\n",
    "                gamma=self.config.SVM_GAMMA,\n",
    "                random_state=self.config.RANDOM_STATE,\n",
    "                probability=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Cross-validation\n",
    "        print(\"ğŸ“Š Performing cross-validation...\")\n",
    "        kfold = KFold(n_splits=self.config.K_FOLDS, shuffle=True, random_state=self.config.RANDOM_STATE)\n",
    "        cv_scores = cross_val_score(svm_pipeline, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "\n",
    "        print(f\"ğŸ“ˆ Cross-validation scores: {cv_scores}\")\n",
    "        print(f\"ğŸ“ˆ Mean CV accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "        # Train final model on entire training set\n",
    "        print(\"ğŸ¯ Training final model...\")\n",
    "        svm_pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = svm_pipeline.predict(X_test)\n",
    "        y_pred_proba = svm_pipeline.predict_proba(X_test)\n",
    "\n",
    "        # Evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, target_names=self.data_loader.classes, output_dict=True)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        print(f\"\\nâœ… Test Accuracy: {accuracy:.4f}\")\n",
    "        print(\"\\nğŸ“‹ Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=self.data_loader.classes))\n",
    "\n",
    "        return {\n",
    "            'model': svm_pipeline,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': report['weighted avg']['precision'],\n",
    "            'recall': report['weighted avg']['recall'],\n",
    "            'f1': report['weighted avg']['f1-score'],\n",
    "            'confusion_matrix': cm,\n",
    "            'cv_scores': cv_scores,\n",
    "            'feature_name': feature_name\n",
    "        }\n",
    "\n",
    "    def plot_confusion_matrix(self, cm, title):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=self.data_loader.classes,\n",
    "                   yticklabels=self.data_loader.classes)\n",
    "        plt.title(f'Confusion Matrix - {title}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize model trainer\n",
    "model_trainer = ModelTrainer(config, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62817882",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# TRAIN AND EVALUATE HOG + SVM\n",
    "\n",
    "# Train HOG + SVM model\n",
    "hog_results = model_trainer.train_svm_model(\n",
    "    X_train_hog, X_test_hog, y_train, y_test, \"HOG\"\n",
    ")\n",
    "\n",
    "# Plot confusion matrix for HOG\n",
    "model_trainer.plot_confusion_matrix(hog_results['confusion_matrix'], \"HOG + SVM\")\n",
    "\n",
    "# Save HOG model\n",
    "joblib.dump(hog_results['model'], 'hog_svm_model.pkl')\n",
    "print(\"ğŸ’¾ HOG + SVM model saved as 'hog_svm_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57d7ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# TRAIN AND EVALUATE LBP + SVM\n",
    "\n",
    "# Train LBP + SVM model\n",
    "lbp_results = model_trainer.train_svm_model(\n",
    "    X_train_lbp, X_test_lbp, y_train, y_test, \"LBP\"\n",
    ")\n",
    "\n",
    "# Plot confusion matrix for LBP\n",
    "model_trainer.plot_confusion_matrix(lbp_results['confusion_matrix'], \"LBP + SVM\")\n",
    "\n",
    "# Save LBP model\n",
    "joblib.dump(lbp_results['model'], 'lbp_svm_model.pkl')\n",
    "print(\"ğŸ’¾ LBP + SVM model saved as 'lbp_svm_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0bd21",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# COMPARISON AND RESULTS VISUALIZATION\n",
    "\n",
    "def compare_results(hog_results, lbp_results):\n",
    "    \"\"\"Compare HOG and LBP performance\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ğŸ“Š PERFORMANCE COMPARISON: HOG vs LBP\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Create comparison table\n",
    "    comparison_data = {\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'CV Mean', 'CV Std'],\n",
    "        'HOG+SVM': [\n",
    "            hog_results['accuracy'],\n",
    "            hog_results['precision'],\n",
    "            hog_results['recall'],\n",
    "            hog_results['f1'],\n",
    "            hog_results['cv_scores'].mean(),\n",
    "            hog_results['cv_scores'].std()\n",
    "        ],\n",
    "        'LBP+SVM': [\n",
    "            lbp_results['accuracy'],\n",
    "            lbp_results['precision'],\n",
    "            lbp_results['recall'],\n",
    "            lbp_results['f1'],\n",
    "            lbp_results['cv_scores'].mean(),\n",
    "            lbp_results['cv_scores'].std()\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    print(df_comparison.round(4).to_string(index=False))\n",
    "\n",
    "    # Visual comparison\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    hog_values = [hog_results['accuracy'], hog_results['precision'],\n",
    "                 hog_results['recall'], hog_results['f1']]\n",
    "    lbp_values = [lbp_results['accuracy'], lbp_results['precision'],\n",
    "                 lbp_results['recall'], lbp_results['f1']]\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "    # Bar chart comparison\n",
    "    bars1 = ax[0].bar(x - width/2, hog_values, width, label='HOG+SVM', alpha=0.8, color='skyblue')\n",
    "    bars2 = ax[0].bar(x + width/2, lbp_values, width, label='LBP+SVM', alpha=0.8, color='lightcoral')\n",
    "\n",
    "    ax[0].set_xlabel('Metrics')\n",
    "    ax[0].set_ylabel('Scores')\n",
    "    ax[0].set_title('Performance Comparison: HOG+SVM vs LBP+SVM')\n",
    "    ax[0].set_xticks(x)\n",
    "    ax[0].set_xticklabels(metrics)\n",
    "    ax[0].legend()\n",
    "    ax[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax[0].annotate(f'{height:.3f}',\n",
    "                      xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                      xytext=(0, 3),\n",
    "                      textcoords=\"offset points\",\n",
    "                      ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax[0].annotate(f'{height:.3f}',\n",
    "                      xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                      xytext=(0, 3),\n",
    "                      textcoords=\"offset points\",\n",
    "                      ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # Cross-validation comparison\n",
    "    cv_data = [hog_results['cv_scores'], lbp_results['cv_scores']]\n",
    "    ax[1].boxplot(cv_data, labels=['HOG+SVM', 'LBP+SVM'])\n",
    "    ax[1].set_title('Cross-Validation Accuracy Distribution')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Determine best method\n",
    "    if hog_results['accuracy'] > lbp_results['accuracy']:\n",
    "        best_method = \"HOG+SVM\"\n",
    "        improvement = hog_results['accuracy'] - lbp_results['accuracy']\n",
    "        print(f\"\\nğŸ¯ BEST PERFORMING METHOD: {best_method}\")\n",
    "        print(f\"ğŸ“ˆ Improvement over LBP+SVM: {improvement:.4f} ({improvement*100:.2f}%)\")\n",
    "    else:\n",
    "        best_method = \"LBP+SVM\"\n",
    "        improvement = lbp_results['accuracy'] - hog_results['accuracy']\n",
    "        print(f\"\\nğŸ¯ BEST PERFORMING METHOD: {best_method}\")\n",
    "        print(f\"ğŸ“ˆ Improvement over HOG+SVM: {improvement:.4f} ({improvement*100:.2f}%)\")\n",
    "\n",
    "    return df_comparison\n",
    "\n",
    "# Perform comparison\n",
    "df_comparison = compare_results(hog_results, lbp_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7390a521",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# SAVE RESULTS AND GENERATE FINAL REPORT\n",
    "\n",
    "# Save all results\n",
    "results = {\n",
    "    'hog_results': hog_results,\n",
    "    'lbp_results': lbp_results,\n",
    "    'comparison': df_comparison.to_dict(),\n",
    "    'config': {\n",
    "        'hog_orientations': config.HOG_ORIENTATIONS,\n",
    "        'hog_pixels_per_cell': config.HOG_PIXELS_PER_CELL,\n",
    "        'hog_cells_per_block': config.HOG_CELLS_PER_BLOCK,\n",
    "        'lbp_radius': config.LBP_RADIUS,\n",
    "        'lbp_n_points': config.LBP_N_POINTS,\n",
    "        'svm_kernel': config.SVM_KERNEL,\n",
    "        'svm_c': config.SVM_C\n",
    "    }\n",
    "}\n",
    "\n",
    "joblib.dump(results, 'classification_results.pkl')\n",
    "print(\"ğŸ’¾ Complete results saved as 'classification_results.pkl'\")\n",
    "\n",
    "# Save feature visualizations\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# HOG example\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(X_train[0])\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(hog_train_vis[0], cmap='gray')\n",
    "plt.title('HOG Features')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(lbp_train_vis[0][0], cmap='gray')\n",
    "plt.title('LBP Features (Channel 0)')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¾ Feature visualization saved as 'feature_visualization.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a1ee1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# FINAL SUMMARY REPORT\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ğŸ‰ PROJECT COMPLETED SUCCESSFULLY! - FINAL SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š DATASET SUMMARY:\")\n",
    "print(f\"   - Training images: {X_train.shape[0]}\")\n",
    "print(f\"   - Test images: {X_test.shape[0]}\")\n",
    "print(f\"   - Classes: {data_loader.classes}\")\n",
    "print(f\"   - Image size: {X_train[0].shape}\")\n",
    "\n",
    "print(f\"\\nğŸ”§ FEATURE EXTRACTION SETTINGS:\")\n",
    "print(f\"   - HOG orientations: {config.HOG_ORIENTATIONS}\")\n",
    "print(f\"   - HOG pixels per cell: {config.HOG_PIXELS_PER_CELL}\")\n",
    "print(f\"   - HOG cells per block: {config.HOG_CELLS_PER_BLOCK}\")\n",
    "print(f\"   - LBP radius: {config.LBP_RADIUS}\")\n",
    "print(f\"   - LBP points: {config.LBP_N_POINTS}\")\n",
    "\n",
    "print(f\"\\nğŸ¤– MODEL SETTINGS:\")\n",
    "print(f\"   - SVM kernel: {config.SVM_KERNEL}\")\n",
    "print(f\"   - SVM C parameter: {config.SVM_C}\")\n",
    "print(f\"   - Cross-validation folds: {config.K_FOLDS}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ FINAL RESULTS:\")\n",
    "print(f\"   - HOG + SVM Accuracy: {hog_results['accuracy']:.4f}\")\n",
    "print(f\"   - LBP + SVM Accuracy: {lbp_results['accuracy']:.4f}\")\n",
    "\n",
    "if hog_results['accuracy'] > lbp_results['accuracy']:\n",
    "    print(f\"   - ğŸ† HOG features performed better by {hog_results['accuracy'] - lbp_results['accuracy']:.4f}\")\n",
    "else:\n",
    "    print(f\"   - ğŸ† LBP features performed better by {lbp_results['accuracy'] - hog_results['accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ SAVED FILES:\")\n",
    "print(\"   - hog_svm_model.pkl (Trained HOG+SVM model)\")\n",
    "print(\"   - lbp_svm_model.pkl (Trained LBP+SVM model)\")\n",
    "print(\"   - classification_results.pkl (All results and metrics)\")\n",
    "print(\"   - feature_visualization.png (Feature visualization)\")\n",
    "\n",
    "print(f\"\\nğŸš€ NEXT STEPS:\")\n",
    "print(\"   - Download the saved files from Colab\")\n",
    "print(\"   - Add them to your GitHub repository\")\n",
    "print(\"   - Update README.md with your results\")\n",
    "print(\"   - Consider experimenting with different parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dfe1f9",
   "metadata": {},
   "source": [
    "======================================================================\n",
    "ğŸ‰ PROJECT COMPLETED SUCCESSFULLY! - FINAL SUMMARY\n",
    "======================================================================\n",
    "\n",
    "ğŸ“Š DATASET SUMMARY:\n",
    "   - Training images: 688\n",
    "   - Test images: 150\n",
    "   - Classes: ['city', 'face', 'green', 'office', 'sea']\n",
    "   - Image size: (128, 128, 3)\n",
    "\n",
    "ğŸ”§ FEATURE EXTRACTION SETTINGS:\n",
    "   - HOG orientations: 9\n",
    "   - HOG pixels per cell: (8, 8)\n",
    "   - HOG cells per block: (2, 2)\n",
    "   - LBP radius: 3\n",
    "   - LBP points: 24\n",
    "\n",
    "ğŸ¤– MODEL SETTINGS:\n",
    "   - SVM kernel: rbf\n",
    "   - SVM C parameter: 1.0\n",
    "   - Cross-validation folds: 5\n",
    "\n",
    "ğŸ“ˆ FINAL RESULTS:\n",
    "   - HOG + SVM Accuracy: 0.7867\n",
    "   - LBP + SVM Accuracy: 0.7000\n",
    "   - ğŸ† HOG features performed better by 0.0867\n",
    "\n",
    "ğŸ’¾ SAVED FILES:\n",
    "   - hog_svm_model.pkl (Trained HOG+SVM model)\n",
    "   - lbp_svm_model.pkl (Trained LBP+SVM model)\n",
    "   - classification_results.pkl (All results and metrics)\n",
    "   - feature_visualization.png (Feature visualization)\n",
    "\n",
    "ğŸš€ NEXT STEPS:\n",
    "   - Download the saved files from Colab\n",
    "   - Add them to your GitHub repository\n",
    "   - Update README.md with your results\n",
    "   - Consider experimenting with different parameters"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
